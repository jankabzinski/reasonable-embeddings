{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "\n",
    "def max_difference_loss(output, target):\n",
    "    # Assuming output and target are 2D tensors (batch_size, num_elements)\n",
    "    diff = T.abs(output - target)\n",
    "    max_diff= T.max(diff)\n",
    "    return T.max(max_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.9033, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.3052, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(0.6436, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0466, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(0.1739, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0136, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(0.3061, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(0.1363, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(0.0943, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "not_nn = nn.Linear(10, 10, bias=False)\n",
    "not_nn2 = nn.Linear(10, 10, bias=False)\n",
    "\n",
    "\n",
    "optimizer = T.optim.AdamW(not_nn.parameters(), 0.0001)\n",
    "optimizer2 = T.optim.AdamW(not_nn2.parameters(), 0.0001)\n",
    "for i in range(30000):\n",
    "    optimizer.zero_grad()\n",
    "    optimizer2.zero_grad()\n",
    "    input1 = T.rand(10)\n",
    "\n",
    "    loss = (max_difference_loss(input1, not_nn(not_nn(input1))))\n",
    "    loss2 = (F.mse_loss(input1, not_nn2(not_nn2(input1))))\n",
    "\n",
    "\n",
    "    if i % 5000==0:\n",
    "        print(\"loss: \", loss)\n",
    "        print('loss2: ',loss2)\n",
    "    loss.backward()\n",
    "    loss2.backward()\n",
    "    optimizer.step()\n",
    "    optimizer2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3165/1596761678.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x= F.softmax(and_nn4(im_mod(input1,input1)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(1.0892, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.2561, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.5601, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0283, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.1094, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0368, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0051, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0507, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0155, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0036, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0574, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0209, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0023, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0532, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0173, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0030, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0634, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0247, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0014, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0949, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0326, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0013, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0205, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(4.6531e-05, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0064, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0009, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0352, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0101, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0007, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0189, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0073, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0007, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0204, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(8.8689e-05, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0081, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0005, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0482, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0122, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0016, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0281, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(3.8793e-05, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0068, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0004, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0169, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(3.0138e-05, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0073, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0006, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0326, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(2.2026e-05, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0063, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0010, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0350, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(2.7490e-05, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0104, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0005, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0140, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(7.6319e-06, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0028, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0010, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0193, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(1.5777e-05, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0056, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0007, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0444, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(4.4990e-06, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0098, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0006, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0242, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(1.8639e-06, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0058, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0009, grad_fn=<SubBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from src.reasoner_mod import *\n",
    "\n",
    "reasoner = ModifiedReasonerHead(emb_size=10, hidden_size=16)\n",
    "reasoner2 = ModifiedReasonerHead(emb_size=10, hidden_size=16)\n",
    "reasoner3 = ModifiedReasonerHead(emb_size=10, hidden_size=16)\n",
    "reasoner4 = ModifiedReasonerHead(emb_size=10, hidden_size=16)\n",
    "\n",
    "and_nn = reasoner.and_nn\n",
    "and_nn2 = reasoner2.and_nn\n",
    "and_nn3 = reasoner3.and_nn\n",
    "and_nn4 = reasoner4.and_nn\n",
    "optimizer = T.optim.AdamW(and_nn.parameters(), 0.0001)\n",
    "optimizer2 = T.optim.AdamW(and_nn2.parameters(), 0.0001)\n",
    "optimizer3 = T.optim.AdamW(and_nn3.parameters(), 0.0001)\n",
    "optimizer4 = T.optim.AdamW(and_nn4.parameters(), 0.0001)\n",
    "for i in range(100000):\n",
    "    optimizer.zero_grad()\n",
    "    optimizer2.zero_grad()\n",
    "    optimizer3.zero_grad()\n",
    "    optimizer4.zero_grad()\n",
    "    input1 = T.rand(10)\n",
    "\n",
    "    loss = (max_difference_loss(input1, and_nn(im_mod(input1, input1))))\n",
    "\n",
    "    loss2 = (F.mse_loss(input1, and_nn2(im_mod(input1, input1))))\n",
    "\n",
    "    loss3 = F.l1_loss(input1, and_nn3(im_mod(input1,input1)))\n",
    "    x= F.softmax(and_nn4(im_mod(input1,input1)))\n",
    "    loss4 = torch.max(x) - torch.mean(x)\n",
    "\n",
    "    if i % 5000==0:\n",
    "        print(\"loss: \", loss)\n",
    "        print('loss2: ',loss2)\n",
    "        print('loss3', loss3)\n",
    "        print('loss4', loss4)\n",
    "        print()\n",
    "\n",
    "    loss.backward()\n",
    "    loss2.backward()\n",
    "    loss3.backward()\n",
    "    loss4.backward()\n",
    "    optimizer.step()\n",
    "    optimizer2.step()\n",
    "    optimizer3.step()\n",
    "    optimizer4.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8047, 0.4910, 0.0637, 0.3009, 0.5346, 0.2209, 0.9889, 0.7555, 0.1164,\n",
      "        0.4491])\n",
      "\n",
      "tensor(0.0205, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0020, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0082, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "input1 = T.rand(10)\n",
    "\n",
    "print(input1)\n",
    "print()\n",
    "output1 = and_nn(im_mod(input1, input1))\n",
    "output2 = and_nn2(im_mod(input1, input1))\n",
    "output3 = and_nn3(im_mod(input1, input1))\n",
    "\n",
    "print(max_difference_loss(output1, input1))\n",
    "print(max_difference_loss(output2, input1))\n",
    "print(max_difference_loss(output3, input1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = trained_reasoner.top_concept[0]\n",
    "bot = trained_reasoner.bot_concept[0]\n",
    "optimizer = T.optim.AdamW( trained_reasoner.and_nn.parameters(), 0.0001)\n",
    "losses=[]\n",
    "for i in range(15000):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    encoder = encoders[int(np.round(random() * (len(encoders ) - 1) , 0))]\n",
    "    input1 = encoder.concepts[ int(np.round( random() * encoder.n_concepts , 0)-1) ] \n",
    "\n",
    "    encoder = encoders[int(np.round(random() * (len(encoders ) - 1) , 0))]\n",
    "    input2 = encoder.concepts[ int(np.round( random() * encoder.n_concepts , 0)-1) ] \n",
    "\n",
    "    encoder = encoders[int(np.round(random() * (len(encoders ) - 1) , 0))]\n",
    "    input3 = encoder.concepts[ int(np.round( random() * encoder.n_concepts , 0) - 1) ] \n",
    "\n",
    "    loss = F.mse_loss(input1, trained_reasoner.and_nn(im_mod(input1,input1)))\n",
    "    loss += F.mse_loss(input2, trained_reasoner.and_nn(im_mod(input2,input2)))\n",
    "    loss += F.mse_loss(input3, trained_reasoner.and_nn(im_mod(input3,input3)))\n",
    "\n",
    "    loss += F.mse_loss(trained_reasoner.and_nn(im_mod(input1, trained_reasoner.and_nn(im_mod(input2, input3)))) , trained_reasoner.and_nn( im_mod (trained_reasoner.and_nn(im_mod(input1,input2)), input3 ) ))\n",
    "    loss += F.mse_loss(trained_reasoner.and_nn(im_mod(input2, trained_reasoner.and_nn(im_mod(input1, input3)))) , trained_reasoner.and_nn( im_mod (trained_reasoner.and_nn(im_mod(input1,input2)), input3 ) ))\n",
    "    loss += F.mse_loss(trained_reasoner.and_nn(im_mod(input3, trained_reasoner.and_nn(im_mod(input1, input2)))) , trained_reasoner.and_nn( im_mod (trained_reasoner.and_nn(im_mod(input3,input2)), input1 ) ))\n",
    "    \n",
    "    loss += F.mse_loss(trained_reasoner.and_nn(im_mod(input1,input3)) , trained_reasoner.and_nn(im_mod(input3,input1)))\n",
    "    loss += F.mse_loss(trained_reasoner.and_nn(im_mod(input3,input2)) , trained_reasoner.and_nn(im_mod(input2,input3)))\n",
    "    loss += F.mse_loss(trained_reasoner.and_nn(im_mod(input2,input1)) , trained_reasoner.and_nn(im_mod(input1,input2)))\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if i%5000==0:\n",
    "        print(\"loss: \", loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_reasoner.load_state_dict(torch.load('local/out/exp/20240630T144632/reasoner.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
