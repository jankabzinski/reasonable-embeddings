{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "\n",
    "def max_difference_loss(output, target):\n",
    "    # Assuming output and target are 2D tensors (batch_size, num_elements)\n",
    "    diff = T.abs(output - target)\n",
    "    max_diff= T.max(diff)\n",
    "    return T.max(max_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.9033, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.3052, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(0.6436, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0466, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(0.1739, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0136, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(0.3061, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(0.1363, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "loss:  tensor(0.0943, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "not_nn = nn.Linear(10, 10, bias=False)\n",
    "not_nn2 = nn.Linear(10, 10, bias=False)\n",
    "\n",
    "\n",
    "optimizer = T.optim.AdamW(not_nn.parameters(), 0.0001)\n",
    "optimizer2 = T.optim.AdamW(not_nn2.parameters(), 0.0001)\n",
    "for i in range(30000):\n",
    "    optimizer.zero_grad()\n",
    "    optimizer2.zero_grad()\n",
    "    input1 = T.rand(10)\n",
    "\n",
    "    loss = (max_difference_loss(input1, not_nn(not_nn(input1))))\n",
    "    loss2 = (F.mse_loss(input1, not_nn2(not_nn2(input1))))\n",
    "\n",
    "\n",
    "    if i % 5000==0:\n",
    "        print(\"loss: \", loss)\n",
    "        print('loss2: ',loss2)\n",
    "    loss.backward()\n",
    "    loss2.backward()\n",
    "    optimizer.step()\n",
    "    optimizer2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3165/1596761678.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x= F.softmax(and_nn4(im_mod(input1,input1)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(1.0892, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.2561, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.5601, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0283, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.1094, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0368, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0051, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0507, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0155, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0036, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0574, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0209, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0023, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0532, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0173, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0030, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0634, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0247, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0014, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0949, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0326, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0013, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0205, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(4.6531e-05, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0064, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0009, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0352, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0101, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0007, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0189, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0073, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0007, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0204, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(8.8689e-05, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0081, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0005, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0482, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0122, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0016, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0281, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(3.8793e-05, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0068, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0004, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0169, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(3.0138e-05, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0073, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0006, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0326, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(2.2026e-05, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0063, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0010, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0350, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(2.7490e-05, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0104, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0005, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0140, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(7.6319e-06, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0028, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0010, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0193, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(1.5777e-05, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0056, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0007, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0444, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(4.4990e-06, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0098, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0006, grad_fn=<SubBackward0>)\n",
      "\n",
      "loss:  tensor(0.0242, grad_fn=<MaxBackward1>)\n",
      "loss2:  tensor(1.8639e-06, grad_fn=<MseLossBackward0>)\n",
      "loss3 tensor(0.0058, grad_fn=<L1LossBackward0>)\n",
      "loss4 tensor(0.0009, grad_fn=<SubBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from src.reasoner_mod import *\n",
    "\n",
    "reasoner = ModifiedReasonerHead(emb_size=10, hidden_size=16)\n",
    "reasoner2 = ModifiedReasonerHead(emb_size=10, hidden_size=16)\n",
    "reasoner3 = ModifiedReasonerHead(emb_size=10, hidden_size=16)\n",
    "reasoner4 = ModifiedReasonerHead(emb_size=10, hidden_size=16)\n",
    "\n",
    "and_nn = reasoner.and_nn\n",
    "and_nn2 = reasoner2.and_nn\n",
    "and_nn3 = reasoner3.and_nn\n",
    "and_nn4 = reasoner4.and_nn\n",
    "optimizer = T.optim.AdamW(and_nn.parameters(), 0.0001)\n",
    "optimizer2 = T.optim.AdamW(and_nn2.parameters(), 0.0001)\n",
    "optimizer3 = T.optim.AdamW(and_nn3.parameters(), 0.0001)\n",
    "optimizer4 = T.optim.AdamW(and_nn4.parameters(), 0.0001)\n",
    "for i in range(100000):\n",
    "    optimizer.zero_grad()\n",
    "    optimizer2.zero_grad()\n",
    "    optimizer3.zero_grad()\n",
    "    optimizer4.zero_grad()\n",
    "    input1 = T.rand(10)\n",
    "\n",
    "    loss = (max_difference_loss(input1, and_nn(im_mod(input1, input1))))\n",
    "\n",
    "    loss2 = (F.mse_loss(input1, and_nn2(im_mod(input1, input1))))\n",
    "\n",
    "    loss3 = F.l1_loss(input1, and_nn3(im_mod(input1,input1)))\n",
    "    x= F.softmax(and_nn4(im_mod(input1,input1)))\n",
    "    loss4 = torch.max(x) - torch.mean(x)\n",
    "\n",
    "    if i % 5000==0:\n",
    "        print(\"loss: \", loss)\n",
    "        print('loss2: ',loss2)\n",
    "        print('loss3', loss3)\n",
    "        print('loss4', loss4)\n",
    "        print()\n",
    "\n",
    "    loss.backward()\n",
    "    loss2.backward()\n",
    "    loss3.backward()\n",
    "    loss4.backward()\n",
    "    optimizer.step()\n",
    "    optimizer2.step()\n",
    "    optimizer3.step()\n",
    "    optimizer4.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8047, 0.4910, 0.0637, 0.3009, 0.5346, 0.2209, 0.9889, 0.7555, 0.1164,\n",
      "        0.4491])\n",
      "\n",
      "tensor(0.0205, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0020, grad_fn=<MaxBackward1>)\n",
      "tensor(0.0082, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "input1 = T.rand(10)\n",
    "\n",
    "print(input1)\n",
    "print()\n",
    "output1 = and_nn(im_mod(input1, input1))\n",
    "output2 = and_nn2(im_mod(input1, input1))\n",
    "output3 = and_nn3(im_mod(input1, input1))\n",
    "\n",
    "print(max_difference_loss(output1, input1))\n",
    "print(max_difference_loss(output2, input1))\n",
    "print(max_difference_loss(output3, input1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = trained_reasoner.top_concept[0]\n",
    "bot = trained_reasoner.bot_concept[0]\n",
    "optimizer = T.optim.AdamW( trained_reasoner.and_nn.parameters(), 0.0001)\n",
    "losses=[]\n",
    "for i in range(15000):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    encoder = encoders[int(np.round(random() * (len(encoders ) - 1) , 0))]\n",
    "    input1 = encoder.concepts[ int(np.round( random() * encoder.n_concepts , 0)-1) ] \n",
    "\n",
    "    encoder = encoders[int(np.round(random() * (len(encoders ) - 1) , 0))]\n",
    "    input2 = encoder.concepts[ int(np.round( random() * encoder.n_concepts , 0)-1) ] \n",
    "\n",
    "    encoder = encoders[int(np.round(random() * (len(encoders ) - 1) , 0))]\n",
    "    input3 = encoder.concepts[ int(np.round( random() * encoder.n_concepts , 0) - 1) ] \n",
    "\n",
    "    loss = F.mse_loss(input1, trained_reasoner.and_nn(im_mod(input1,input1)))\n",
    "    loss += F.mse_loss(input2, trained_reasoner.and_nn(im_mod(input2,input2)))\n",
    "    loss += F.mse_loss(input3, trained_reasoner.and_nn(im_mod(input3,input3)))\n",
    "\n",
    "    loss += F.mse_loss(trained_reasoner.and_nn(im_mod(input1, trained_reasoner.and_nn(im_mod(input2, input3)))) , trained_reasoner.and_nn( im_mod (trained_reasoner.and_nn(im_mod(input1,input2)), input3 ) ))\n",
    "    loss += F.mse_loss(trained_reasoner.and_nn(im_mod(input2, trained_reasoner.and_nn(im_mod(input1, input3)))) , trained_reasoner.and_nn( im_mod (trained_reasoner.and_nn(im_mod(input1,input2)), input3 ) ))\n",
    "    loss += F.mse_loss(trained_reasoner.and_nn(im_mod(input3, trained_reasoner.and_nn(im_mod(input1, input2)))) , trained_reasoner.and_nn( im_mod (trained_reasoner.and_nn(im_mod(input3,input2)), input1 ) ))\n",
    "    \n",
    "    loss += F.mse_loss(trained_reasoner.and_nn(im_mod(input1,input3)) , trained_reasoner.and_nn(im_mod(input3,input1)))\n",
    "    loss += F.mse_loss(trained_reasoner.and_nn(im_mod(input3,input2)) , trained_reasoner.and_nn(im_mod(input2,input3)))\n",
    "    loss += F.mse_loss(trained_reasoner.and_nn(im_mod(input2,input1)) , trained_reasoner.and_nn(im_mod(input1,input2)))\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if i%5000==0:\n",
    "        print(\"loss: \", loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_reasoner.load_state_dict(torch.load('local/out/exp/20240630T144632/reasoner.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\t\t\t# Prawo wyłączonego środka: A ⊔ ¬A = T\n",
    "\t\t# loss += F.mse_loss(top[0], or_nn(im_mod(input1, not_nn(input1))))\n",
    "\t\t# loss += F.mse_loss(top[0], or_nn(im_mod(input2, not_nn(input2))))\n",
    "\t\t# loss += F.mse_loss(top[0], or_nn(im_mod(input3, not_nn(input3))))\n",
    "\n",
    "\t\t# loss += F.mse_loss(top[0], or_nn(im_mod(not_nn(input1), input1)))\n",
    "\t\t# loss += F.mse_loss(top[0], or_nn(im_mod(not_nn(input2), input2)))\n",
    "\t\t# loss += F.mse_loss(top[0], or_nn(im_mod(not_nn(input3), input3)))\n",
    "\n",
    "\t\t# # Prawa de Morgana: ¬(A ⊔ B) = ¬A ⊓ ¬B\n",
    "\t\t# loss += F.mse_loss(not_nn(or_nn(im_mod(input1, input2))), \n",
    "\t\t# \t\t\t\tand_nn(im_mod(not_nn(input1), not_nn(input2))))\n",
    "\t\t# # Przykład 2\n",
    "\t\t# loss += F.mse_loss(not_nn(or_nn(im_mod(input2, input3))), \n",
    "\t\t# \t\t\t\tand_nn(im_mod(not_nn(input2), not_nn(input3))))\n",
    "\t\t# # Przykład 3\n",
    "\t\t# loss += F.mse_loss(not_nn(or_nn(im_mod(input1, input3))), \n",
    "\t\t# \t\t\t\tand_nn(im_mod(not_nn(input1), not_nn(input3))))\n",
    "\n",
    "\t\t# # Obustronnie zanegowana opcja: ¬(A ⊔ B) = ¬A ⊓ ¬B\n",
    "\t\t# # Przykład 1\n",
    "\t\t# loss += F.mse_loss(or_nn(im_mod(input1, input2)), \n",
    "\t\t# \t\t\t\tnot_nn(and_nn(im_mod(not_nn(input1), not_nn(input2)))))\n",
    "\t\t# # Przykład 2\n",
    "\t\t# loss += F.mse_loss(or_nn(im_mod(input2, input3)), \n",
    "\t\t# \t\t\t\tnot_nn(and_nn(im_mod(not_nn(input2), not_nn(input3)))))\n",
    "\t\t# # Przykład 3\n",
    "\t\t# loss += F.mse_loss(or_nn(im_mod(input1, input3)), \n",
    "\t\t# \t\t\t\tnot_nn(and_nn(im_mod(not_nn(input1), not_nn(input3)))))\n",
    "\n",
    "\t\t# # Prawa de Morgana: ¬(A ⊓ B) = ¬A ⊔ ¬B\n",
    "\t\t# # Przykład 1\n",
    "\t\t# loss += F.mse_loss(not_nn(and_nn(im_mod(input1, input2))), \n",
    "\t\t# \t\t\t\tor_nn(im_mod(not_nn(input1), not_nn(input2))))\n",
    "\t\t# # Przykład 2\n",
    "\t\t# loss += F.mse_loss(not_nn(and_nn(im_mod(input2, input3))), \n",
    "\t\t# \t\t\t\tor_nn(im_mod(not_nn(input2), not_nn(input3))))\n",
    "\t\t# # Przykład 3\n",
    "\t\t# loss += F.mse_loss(not_nn(and_nn(im_mod(input1, input3))), \n",
    "\t\t# \t\t\t\tor_nn(im_mod(not_nn(input1), not_nn(input3))))\n",
    "\n",
    "\t\t# # Prawa absorpcji: A ⊔ (A ⊓ B) = A\n",
    "\t\t# loss += F.mse_loss(input1, or_nn(im_mod(input1, and_nn(im_mod(input1, input2)))))\n",
    "\t\t# loss += F.mse_loss(input1, or_nn(im_mod(and_nn(im_mod(input2, input1)), input2)))\n",
    "\n",
    "\t\t# loss += F.mse_loss(input1, or_nn(im_mod(input1, and_nn(im_mod(input1, input3)))))\n",
    "\t\t# loss += F.mse_loss(input1, or_nn(im_mod(and_nn(im_mod(input3, input1)), input3)))\n",
    "\n",
    "\t\t# # Dla input2\n",
    "\t\t# loss += F.mse_loss(input2, or_nn(im_mod(input2, and_nn(im_mod(input2, input1)))))\n",
    "\t\t# loss += F.mse_loss(input2, or_nn(im_mod(and_nn(im_mod(input1, input2)), input1)))\n",
    "\t\t# loss += F.mse_loss(input2, or_nn(im_mod(input2, and_nn(im_mod(input2, input3)))))\n",
    "\t\t# loss += F.mse_loss(input2, or_nn(im_mod(and_nn(im_mod(input3, input2)), input3)))\n",
    "\n",
    "\t\t# # Dla input3\n",
    "\t\t# loss += F.mse_loss(input3, or_nn(im_mod(input3, and_nn(im_mod(input3, input1)))))\n",
    "\t\t# loss += F.mse_loss(input3, or_nn(im_mod(and_nn(im_mod(input1, input3)), input1)))\n",
    "\t\t# loss += F.mse_loss(input3, or_nn(im_mod(input3, and_nn(im_mod(input3, input2)))))\n",
    "\t\t# loss += F.mse_loss(input3, or_nn(im_mod(and_nn(im_mod(input2, input3)), input2)))\n",
    "\n",
    "\t\t# # Prawa dystrybucji: A ⊔ (B ⊓ C) = (A ⊔ B) ⊓ (A ⊔ C)\n",
    "\t\t# loss += F.mse_loss(or_nn(im_mod(input1, and_nn(im_mod(input2, input3)))),\n",
    "\t\t# \t\t\t\tand_nn(im_mod(or_nn(im_mod(input1, input2)), or_nn(im_mod(input1, input3)))))\n",
    "\t\t# loss += F.mse_loss(or_nn(im_mod(input2, and_nn(im_mod(input1, input3)))),\n",
    "        #            and_nn(im_mod(or_nn(im_mod(input2, input1)), or_nn(im_mod(input2, input3)))))\n",
    "\t\t# loss += F.mse_loss(or_nn(im_mod(input3, and_nn(im_mod(input2, input1)))),\n",
    "        #            and_nn(im_mod(or_nn(im_mod(input3, input2)), or_nn(im_mod(input3, input1)))))\n",
    "\n",
    "\t\t# # Właściwości zbioru pustego i pełnego z OR\n",
    "\t\t# loss += F.mse_loss(input1, or_nn(im_mod(input1, bot[0])))\n",
    "\t\t# loss += F.mse_loss(input2, or_nn(im_mod(input2, bot[0])))\n",
    "\t\t# loss += F.mse_loss(input3, or_nn(im_mod(input3, bot[0])))\n",
    "\t\t# # Input po prawej stronie im_mod\n",
    "\t\t# loss += F.mse_loss(input1, or_nn(im_mod(bot[0], input1)))\n",
    "\t\t# loss += F.mse_loss(input2, or_nn(im_mod(bot[0], input2)))\n",
    "\t\t# loss += F.mse_loss(input3, or_nn(im_mod(bot[0], input3)))\n",
    "\n",
    "\t\t# # Input po lewej stronie im_mod\n",
    "\t\t# loss += F.mse_loss(top[0], or_nn(im_mod(input1, top[0])))\n",
    "\t\t# loss += F.mse_loss(top[0], or_nn(im_mod(input2, top[0])))\n",
    "\t\t# loss += F.mse_loss(top[0], or_nn(im_mod(input3, top[0])))\n",
    "\t\t# # Input po prawej stronie im_mod\n",
    "\t\t# loss += F.mse_loss(top[0], or_nn(im_mod(top[0], input1)))\n",
    "\t\t# loss += F.mse_loss(top[0], or_nn(im_mod(top[0], input2)))\n",
    "\t\t# loss += F.mse_loss(top[0], or_nn(im_mod(top[0], input3)))\n",
    "\n",
    "\t\t# # Właściwości kommutatywności i łączności dla OR\n",
    "\t\t# loss += F.mse_loss(or_nn(im_mod(input1, input2)), or_nn(im_mod(input2, input1)))  # A ⊔ B = B ⊔ A\n",
    "\t\t# loss += F.mse_loss(or_nn(im_mod(input3, input1)), or_nn(im_mod(input1, input3)))  # A ⊔ B = B ⊔ A\n",
    "\t\t# loss += F.mse_loss(or_nn(im_mod(input2, input3)), or_nn(im_mod(input3, input2)))  # A ⊔ B = B ⊔ A\n",
    "\n",
    "\t\t# loss += F.mse_loss(or_nn(im_mod(input1, or_nn(im_mod(input2, input3)))),\n",
    "\t\t# \t\t\t\tor_nn(im_mod(or_nn(im_mod(input1, input2)), input3)))  # A ⊔ (B ⊔ C) = (A ⊔ B) ⊔ C\n",
    "\t\t# loss += F.mse_loss(or_nn(im_mod(input3, or_nn(im_mod(input1, input2)))),\n",
    "\t\t# \t\t\t\tor_nn(im_mod(or_nn(im_mod(input1, input3)), input2)))  # A ⊔ (B ⊔ C) = (A ⊔ B) ⊔ C\n",
    "\t\t# loss += F.mse_loss(or_nn(im_mod(input2, or_nn(im_mod(input1, input3)))),\n",
    "\t\t# \t\t\t\tor_nn(im_mod(or_nn(im_mod(input2, input3)), input1)))  # A ⊔ (B ⊔ C) = (A ⊔ B) ⊔ C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not freeze_reasoner:\n",
    "\t\t\t# \treasoner.identity_losses(encoders,freeze_reasoner, one_onto, lr_reasoner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef identity_losses(self, encoders, frozen, lr ,one_onto=False):\n",
    "\t\tand_nn = self.and_nn\n",
    "\t\tbot = self.bot_concept\n",
    "\t\ttop = self.top_concept\n",
    "\t\tsub_nn = self.sub_nn\n",
    "\t\tnot_nn = self.not_nn\n",
    "\t\t# or_nn = self.or_nn \n",
    "\t\toptimizer=T.optim.AdamW\n",
    "\t\toptimizers = []\n",
    "\n",
    "\t\toptimizers.append(optimizer(not_nn.parameters(), lr*2.5))\n",
    "\t\toptimizers.append(optimizer([{'params':not_nn.parameters()},\n",
    "\t\t\t\t\t \t   {'params':top},\n",
    "\t\t\t\t\t\t   {'params':bot}], lr))\n",
    "\t\toptimizers.append(optimizer(and_nn.parameters(), lr))\n",
    "\n",
    "\t\toptimizers.append(optimizer([{'params':and_nn.parameters()},\n",
    "\t\t\t\t\t \t   {'params':top},\n",
    "\t\t\t\t\t\t   {'params':bot}], lr))\n",
    "\n",
    "\t\toptimizers.append(optimizer([{'params':sub_nn.parameters()},\n",
    "\t\t\t\t\t \t   {'params':top},\n",
    "\t\t\t\t\t\t   {'params':bot}], lr))\n",
    "\n",
    "\t\toptimizers.append(optimizer(sub_nn.parameters(), lr))\n",
    "\n",
    "\t\toptimizers.append(optimizer([{'params':sub_nn.parameters()},\n",
    "\t\t\t\t\t \t   {'params':not_nn.parameters()}], lr))\n",
    "\t\t\t\t\n",
    "\t\toptimizers.append(optimizer([{'params':and_nn.parameters()},\n",
    "\t\t\t\t\t\t   {'params':bot},\n",
    "\t\t\t\t\t\t   {'params':not_nn.parameters()}], lr))\n",
    "\n",
    "\t\tfor optim in optimizers:\n",
    "\t\t\toptim.zero_grad()\n",
    "\n",
    "\t\tencoder = encoders[int(np.round(random() * (len(encoders) - 1), 0))]\n",
    "\t\tinput1 = encoder.concepts[int(np.round(random() * encoder.n_concepts, 0) - 1)]\n",
    "\t\tinput2 = encoder.concepts[int(np.round(random() * encoder.n_concepts, 0) - 1)]\n",
    "\t\tinput3 = encoder.concepts[int(np.round(random() * encoder.n_concepts, 0) - 1)]\n",
    "\t\t\n",
    "\t\tloss = 0\n",
    "\n",
    "\t\t#  A = ¬(¬(A))\n",
    "\t\tloss += F.mse_loss(input1, not_nn(not_nn(input1)))\n",
    "\t\tloss += F.mse_loss(input2, not_nn(not_nn(input2)))\n",
    "\t\tloss += F.mse_loss(input3, not_nn(not_nn(input3)))\n",
    "\n",
    "\t\tif not frozen:\n",
    "\t\t\tloss += F.l1_loss(T.matmul(not_nn.weight, not_nn.weight), T.eye(not_nn.weight.shape[1])) * 100\n",
    "\n",
    "\t\tloss.backward()\n",
    "\t\toptimizers[0].step()\n",
    "\t\tfor optim in optimizers:\n",
    "\t\t\toptim.zero_grad()\n",
    "\t\t\n",
    "\t\tloss=0\n",
    "\n",
    "\t\t#  ⊥ = ¬T  \n",
    "\t\tif not frozen:\n",
    "\t\t\tloss += F.l1_loss(bot[0], not_nn(top[0]))\n",
    "\t\t#  T = ¬⊥\n",
    "\t\t\tloss += F.l1_loss(top[0], not_nn(bot[0]))\n",
    "\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizers[1].step()\n",
    "\t\t\tfor optim in optimizers:\n",
    "\t\t\t\toptim.zero_grad()\n",
    "\n",
    "\t\tif one_onto is False:\n",
    "\t\t\t# A ⊓ A = A\n",
    "\t\t\tloss=0\n",
    "\n",
    "\t\t\tloss += F.mse_loss(input1, and_nn(im_mod(input1, input1)))\n",
    "\t\t\tloss += F.mse_loss(input2, and_nn(im_mod(input2, input2)))\n",
    "\t\t\tloss += F.mse_loss(input3, and_nn(im_mod(input3, input3)))\n",
    "\n",
    "\t\t\t# A ⊓ (B ⊓ C) = (A ⊓ B) ⊓ C \n",
    "\t\t\tloss += F.mse_loss(and_nn(im_mod(input1, and_nn(im_mod(input2, input3)))), and_nn(im_mod(and_nn(im_mod(input1, input2)), input3)))\n",
    "\t\t\tloss += F.mse_loss(and_nn(im_mod(input2, and_nn(im_mod(input1, input3)))), and_nn(im_mod(and_nn(im_mod(input1, input2)), input3)))\n",
    "\t\t\tloss += F.mse_loss(and_nn(im_mod(input3, and_nn(im_mod(input1, input2)))), and_nn(im_mod(and_nn(im_mod(input3, input2)), input1)))\n",
    "\n",
    "\t\t\t# A ⊓ B = B ⊓ A\n",
    "\t\t\tloss += F.mse_loss(and_nn(im_mod(input1, input3)), and_nn(im_mod(input3, input1)))\n",
    "\t\t\tloss += F.mse_loss(and_nn(im_mod(input3, input2)), and_nn(im_mod(input2, input3)))\n",
    "\t\t\tloss += F.mse_loss(and_nn(im_mod(input2, input1)), and_nn(im_mod(input1, input2)))\n",
    "\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizers[2].step()\n",
    "\t\t\tfor optim in optimizers:\n",
    "\t\t\t\toptim.zero_grad()\n",
    "\n",
    "\t\t\tloss=0\n",
    "\n",
    "\t\t\t# ⊥ = A ⊓ ¬A\n",
    "\t\t\tloss += F.mse_loss(bot[0], and_nn(im_mod(input1, not_nn(input1))))\n",
    "\t\t\tloss += F.mse_loss(bot[0], and_nn(im_mod(input2, not_nn(input2))))\n",
    "\t\t\tloss += F.mse_loss(bot[0], and_nn(im_mod(input3, not_nn(input3))))\n",
    "\n",
    "\t\t\tloss += F.mse_loss(bot[0], and_nn(im_mod(not_nn(input1), input1)))\n",
    "\t\t\tloss += F.mse_loss(bot[0], and_nn(im_mod(not_nn(input2), input2)))\n",
    "\t\t\tloss += F.mse_loss(bot[0], and_nn(im_mod(not_nn(input3), input3)))\n",
    "\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizers[7].step()\n",
    "\t\t\tfor optim in optimizers:\n",
    "\t\t\t\toptim.zero_grad()\n",
    "\n",
    "\t\t\tloss =0\n",
    "\t\t\t# A = A ⊓ T\n",
    "\t\t\tloss += F.mse_loss(input1, and_nn(im_mod(input1, top[0])))\n",
    "\t\t\tloss += F.mse_loss(input1, and_nn(im_mod(input2, top[0])))\n",
    "\t\t\tloss += F.mse_loss(input3, and_nn(im_mod(input3, top[0])))\n",
    "\n",
    "\t\t\tloss += F.mse_loss(input2, and_nn(im_mod(top[0], input1)))\n",
    "\t\t\tloss += F.mse_loss(input2, and_nn(im_mod(top[0], input2)))\n",
    "\t\t\tloss += F.mse_loss(input2, and_nn(im_mod(top[0], input3)))\n",
    "\n",
    "\t\t\t# ⊥ = A ⊓ ⊥\n",
    "\t\t\tloss += F.mse_loss(bot[0], and_nn(im_mod(input1, bot[0])))\n",
    "\t\t\tloss += F.mse_loss(bot[0], and_nn(im_mod(input2, bot[0])))\n",
    "\t\t\tloss += F.mse_loss(bot[0], and_nn(im_mod(input3, bot[0])))\n",
    "\n",
    "\t\t\tloss += F.mse_loss(bot[0], and_nn(im_mod(bot[0], input1)))\n",
    "\t\t\tloss += F.mse_loss(bot[0], and_nn(im_mod(bot[0], input2)))\n",
    "\t\t\tloss += F.mse_loss(bot[0], and_nn(im_mod(bot[0], input3)))\n",
    "\n",
    "\t\t\tif not frozen:\n",
    "\t\t\t\tloss += F.mse_loss(bot[0], and_nn(im_mod(top[0], bot[0])))\n",
    "\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizers[3].step()\n",
    "\t\t\tfor optim in optimizers:\n",
    "\t\t\t\toptim.zero_grad()\n",
    "\n",
    "\t\t#  A ⊑ T -> True\n",
    "\t\tloss=0\n",
    "\t\tloss += (1 - T.sigmoid(sub_nn(im_mod(input1, top[0])))).sum()\n",
    "\t\tloss += (1 - T.sigmoid(sub_nn(im_mod(input2, top[0])))).sum()\n",
    "\t\tloss += (1 - T.sigmoid(sub_nn(im_mod(input3, top[0])))).sum()\n",
    "\t\tif not frozen:\n",
    "\t\t\tloss += (1 - T.sigmoid(sub_nn(im_mod(bot[0], top[0])))).sum()\n",
    "\n",
    "\t\t#  ⊥ ⊑ A -> True\n",
    "\t\tloss += (1 - T.sigmoid(sub_nn(im_mod(bot[0], input1)))).sum()\n",
    "\t\tloss += (1 - T.sigmoid(sub_nn(im_mod(bot[0], input2)))).sum()\n",
    "\t\tloss += (1 - T.sigmoid(sub_nn(im_mod(bot[0], input3)))).sum()\n",
    "\n",
    "\t\t#  ⊥ ⊑ ⊥ -> True\n",
    "\t\tif not frozen:\n",
    "\t\t\tloss += (1 - T.sigmoid(sub_nn(im_mod(bot[0], bot[0])))).sum()\t\n",
    "\t\t\n",
    "\t\tloss.backward()\n",
    "\t\toptimizers[4].step()\n",
    "\t\tfor optim in optimizers:\n",
    "\t\t\toptim.zero_grad()\n",
    "\n",
    "\n",
    "\t\t#  A ⊑ A -> True\n",
    "\t\tloss=0\n",
    "\t\tloss += (1 - T.sigmoid(sub_nn(im_mod(input1, input1)))).sum()\n",
    "\t\tloss += (1 - T.sigmoid(sub_nn(im_mod(input2, input2)))).sum()\n",
    "\t\tloss += (1 - T.sigmoid(sub_nn(im_mod(input3, input3)))).sum()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizers[5].step()\n",
    "\t\tfor optim in optimizers:\n",
    "\t\t\toptim.zero_grad()\n",
    "\t\t#  A ⊑ ¬A -> False\n",
    "\t\tloss=0\n",
    "\t\tloss+= T.sigmoid(sub_nn(im_mod(input3, not_nn(input3)))).sum()\n",
    "\t\tloss+= T.sigmoid(sub_nn(im_mod(input2, not_nn(input2)))).sum()\n",
    "\t\tloss+= T.sigmoid(sub_nn(im_mod(input1, not_nn(input1)))).sum()\n",
    "\n",
    "\t\tloss.backward()\n",
    "\t\toptimizers[6].step()\n",
    "\t\tfor optim in optimizers:\n",
    "\t\t\toptim.zero_grad()\n",
    "\n",
    "\t\t\n",
    "\t\treturn loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
